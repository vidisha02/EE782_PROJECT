{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:32:32.594175Z",
     "iopub.status.busy": "2025-11-20T21:32:32.593348Z",
     "iopub.status.idle": "2025-11-20T21:32:38.188226Z",
     "shell.execute_reply": "2025-11-20T21:32:38.187460Z",
     "shell.execute_reply.started": "2025-11-20T21:32:32.594126Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies already installed!\n",
      "   NumPy: 2.2.6\n",
      "   PyTorch: 2.6.0+cu124\n",
      "   Transformers: 4.36.0\n",
      "\n",
      "üíæ Free Space: 1495 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "# Check if this is first run\n",
    "\n",
    "first_run = not os.path.exists('/tmp/deps_installed')\n",
    "\n",
    "if first_run:\n",
    "    print(\"Installing dependencies...\")\n",
    "\n",
    "    # Fix for numpy conflicts\n",
    "    !pip install -q --force-reinstall \"numpy==1.26.4\"\n",
    "\n",
    "    # Install required packages\n",
    "    !pip install -q transformers==4.36.0 timm==0.9.12 accelerate\n",
    "    !pip install -q torch torchvision tqdm pillow opencv-python-headless\n",
    "\n",
    "    # Mark installation complete\n",
    "    with open('/tmp/deps_installed', 'w') as f:\n",
    "        f.write('done')\n",
    "\n",
    "    print(\"üîÑ Restarting kernel...\")\n",
    "    os.kill(os.getpid(), 9)\n",
    "else:\n",
    "    print(\"‚úÖ Dependencies already installed!\")\n",
    "\n",
    "    \n",
    "    # Verify versions\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import transformers\n",
    "    print(f\"   NumPy: {np.__version__}\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    print(f\"\\nüíæ Free Space: {free // (2**30)} GB\")\n",
    "    \n",
    "    # Clean up flag for next run\n",
    "    if os.path.exists('/tmp/deps_installed'):\n",
    "        os.remove('/tmp/deps_installed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:32:41.904415Z",
     "iopub.status.busy": "2025-11-20T21:32:41.903781Z",
     "iopub.status.idle": "2025-11-20T21:32:49.334215Z",
     "shell.execute_reply": "2025-11-20T21:32:49.333375Z",
     "shell.execute_reply.started": "2025-11-20T21:32:41.904388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration, T5Config, T5Tokenizer\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:32:51.787126Z",
     "iopub.status.busy": "2025-11-20T21:32:51.786084Z",
     "iopub.status.idle": "2025-11-20T21:33:05.511423Z",
     "shell.execute_reply": "2025-11-20T21:33:05.510542Z",
     "shell.execute_reply.started": "2025-11-20T21:32:51.787097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading A-OKVQA from parquet files...\n",
      "Found 3 parquet files\n",
      "  Train files: 2\n",
      "  Val files: 1\n",
      "\n",
      "üîç Inspecting parquet schema...\n",
      "Columns: ['image', 'question_id', 'question', 'choices', 'correct_choice_idx', 'direct_answers', 'difficult_direct_answer', 'rationales']\n",
      "Sample image field type: <class 'dict'>\n",
      "\n",
      "Converting training data...\n",
      "  Reading train-00000-of-00002-c1d24de3bacb5e0c.parquet...\n",
      "  Reading train-00001-of-00002-6b4f3abe2dc385d0.parquet...\n",
      "‚úÖ Train: 17056 samples\n",
      "\n",
      "Converting validation data...\n",
      "  Reading validation-00000-of-00001-b2bd0de231b6326a.parquet...\n",
      "‚úÖ Val: 1145 samples\n",
      "\n",
      "‚úÖ A-OKVQA dataset ready!\n",
      "\n",
      "‚ö†Ô∏è  Note: Images are embedded in parquet as bytes\n",
      "   We'll extract them during feature extraction\n",
      "\n",
      "üìä Sample data:\n",
      "  Question: What is in the motorcyclist's mouth?\n",
      "  Choices: ['toothpick', 'food', 'popsicle stick', 'cigarette']\n",
      "  Correct: 3\n",
      "  Rationales: 1 provided\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Load A-OKVQA from Parquet Files\n",
    "# ============================================================================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "for dir_path in ['data/aokvqa', 'data/coco', 'models', 'outputs']:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Loading A-OKVQA from parquet files...\")\n",
    "\n",
    "# Find parquet files\n",
    "parquet_paths = glob.glob('/kaggle/input/*/train-*.parquet') + glob.glob('/kaggle/input/*/validation-*.parquet')\n",
    "\n",
    "if not parquet_paths:\n",
    "    print(\"‚ùå No parquet files found\")\n",
    "    raise Exception(\"Parquet files not found\")\n",
    "\n",
    "print(f\"Found {len(parquet_paths)} parquet files\")\n",
    "\n",
    "train_files = sorted([f for f in parquet_paths if 'train-' in f])\n",
    "val_files = sorted([f for f in parquet_paths if 'validation-' in f])\n",
    "\n",
    "print(f\"  Train files: {len(train_files)}\")\n",
    "print(f\"  Val files: {len(val_files)}\")\n",
    "\n",
    "# Inspect schema\n",
    "print(\"\\nüîç Inspecting parquet schema...\")\n",
    "sample_df = pd.read_parquet(parquet_paths[0])\n",
    "print(f\"Columns: {list(sample_df.columns)}\")\n",
    "print(f\"Sample image field type: {type(sample_df.iloc[0]['image'])}\")\n",
    "\n",
    "def convert_parquet_to_json(parquet_files, output_file):\n",
    "    \"\"\"Read parquet files and convert to JSON format\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for pq_file in parquet_files:\n",
    "        print(f\"  Reading {os.path.basename(pq_file)}...\")\n",
    "        df = pd.read_parquet(pq_file)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Get question_id\n",
    "            question_id = str(row.get('question_id', idx))\n",
    "            \n",
    "            # Get image_id from image field\n",
    "            image_field = row.get('image', {})\n",
    "            if isinstance(image_field, dict):\n",
    "                # Try to get image_id from bytes or metadata\n",
    "                # For A-OKVQA, we need to extract COCO image_id\n",
    "                # Usually stored in the dict or we can derive from question_id\n",
    "                # For now, use a placeholder - will be mapped later\n",
    "                image_id = hash(question_id) % 1000000  # Temp placeholder\n",
    "            else:\n",
    "                image_id = 0\n",
    "            \n",
    "            # Get other fields\n",
    "            question = str(row.get('question', ''))\n",
    "            choices = list(row.get('choices', []))\n",
    "            correct_idx = int(row.get('correct_choice_idx', -1))\n",
    "            \n",
    "            # Get rationales - handle as list\n",
    "            rationales_raw = row.get('rationales', [])\n",
    "            if isinstance(rationales_raw, list):\n",
    "                rationales = rationales_raw\n",
    "            elif rationales_raw is None or (isinstance(rationales_raw, float) and pd.isna(rationales_raw)):\n",
    "                rationales = []\n",
    "            else:\n",
    "                rationales = [str(rationales_raw)]\n",
    "            \n",
    "            item = {\n",
    "                'question_id': question_id,\n",
    "                'image_id': image_id,\n",
    "                'question': question,\n",
    "                'choices': choices,\n",
    "                'correct_choice_idx': correct_idx,\n",
    "                'rationales': rationales\n",
    "            }\n",
    "            all_data.append(item)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_data, f)\n",
    "    \n",
    "    return len(all_data)\n",
    "\n",
    "# Convert files\n",
    "if train_files:\n",
    "    print(\"\\nConverting training data...\")\n",
    "    train_count = convert_parquet_to_json(train_files, 'data/aokvqa/aokvqa_v1p0_train.json')\n",
    "    print(f\"‚úÖ Train: {train_count} samples\")\n",
    "\n",
    "if val_files:\n",
    "    print(\"\\nConverting validation data...\")\n",
    "    val_count = convert_parquet_to_json(val_files, 'data/aokvqa/aokvqa_v1p0_val.json')\n",
    "    print(f\"‚úÖ Val: {val_count} samples\")\n",
    "\n",
    "print(\"\\n‚úÖ A-OKVQA dataset ready!\")\n",
    "\n",
    "# Now we need to map image_ids to COCO IDs\n",
    "# The image field contains the actual image bytes\n",
    "# We'll need to save these images or use COCO dataset mapping\n",
    "print(\"\\n‚ö†Ô∏è  Note: Images are embedded in parquet as bytes\")\n",
    "print(\"   We'll extract them during feature extraction\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìä Sample data:\")\n",
    "with open('data/aokvqa/aokvqa_v1p0_val.json', 'r') as f:\n",
    "    sample = json.load(f)[0]\n",
    "    print(f\"  Question: {sample['question']}\")\n",
    "    print(f\"  Choices: {sample['choices']}\")\n",
    "    print(f\"  Correct: {sample['correct_choice_idx']}\")\n",
    "    print(f\"  Rationales: {len(sample['rationales'])} provided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:02:08.723764Z",
     "iopub.status.busy": "2025-11-19T09:02:08.723099Z",
     "iopub.status.idle": "2025-11-19T09:06:43.123595Z",
     "shell.execute_reply": "2025-11-19T09:06:43.122368Z",
     "shell.execute_reply.started": "2025-11-19T09:02:08.723737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: EXTRACTING VISION FEATURES\n",
      "============================================================\n",
      "\n",
      "üîç Extracting features for: data/aokvqa/train_1k.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc8c5599011478cadfd2ea96c35f0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00000-of-00002-c1d24de3bacb5e0c.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:47<00:00, 180.99it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00001-of-00002-6b4f3abe2dc385d0.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:00<00:00, 27195.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 1000 features\n",
      "\n",
      "üîç Extracting features for: data/aokvqa/val_200.json\n",
      "  üìÇ Reading: train-00000-of-00002-c1d24de3bacb5e0c.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:09<00:00, 883.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00001-of-00002-6b4f3abe2dc385d0.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:00<00:00, 27000.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 200 features\n",
      "‚úÖ All features extracted!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: EXTRACT VISION FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: EXTRACTING VISION FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_features_fast(data_file, parquet_files, output_file):\n",
    "    \"\"\"Fast feature extraction with GPU\"\"\"\n",
    "    print(f\"\\nüîç Extracting features for: {data_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    question_ids = {item['question_id']: item['image_id'] for item in data}\n",
    "    \n",
    "    # Load model\n",
    "    model = timm.create_model('vit_large_patch32_384', pretrained=True, num_classes=0).eval().to(device)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for pq_file in parquet_files:\n",
    "        print(f\"  üìÇ Reading: {os.path.basename(pq_file)}\")\n",
    "        df = pd.read_parquet(pq_file)\n",
    "        \n",
    "        for idx in tqdm(range(len(df)), desc=\"Processing\"):\n",
    "            row = df.iloc[idx]\n",
    "            qid = str(row['question_id'])\n",
    "            \n",
    "            if qid not in question_ids:\n",
    "                continue\n",
    "            \n",
    "            img_id = str(question_ids[qid])\n",
    "            if img_id in features:\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            img = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feat = model.forward_features(img_tensor)\n",
    "            \n",
    "            features[img_id] = feat[0].cpu().tolist()\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(features, f)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(features)} features\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Get parquet files\n",
    "train_parquets = sorted(glob.glob('/kaggle/input/*/train-*.parquet'))\n",
    "\n",
    "# Extract features\n",
    "if not os.path.exists('data/aokvqa/feat_train_1k.json'):\n",
    "    extract_features_fast('data/aokvqa/train_1k.json', train_parquets, 'data/aokvqa/feat_train_1k.json')\n",
    "\n",
    "if not os.path.exists('data/aokvqa/feat_val_200.json'):\n",
    "    extract_features_fast('data/aokvqa/val_200.json', train_parquets, 'data/aokvqa/feat_val_200.json')\n",
    "\n",
    "print(\"‚úÖ All features extracted!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:14.217586Z",
     "iopub.status.busy": "2025-11-20T21:33:14.217278Z",
     "iopub.status.idle": "2025-11-20T21:33:20.177113Z",
     "shell.execute_reply": "2025-11-20T21:33:20.176173Z",
     "shell.execute_reply.started": "2025-11-20T21:33:14.217564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --force-reinstall numpy==1.26.4\n",
    "# Then manually restart kernel before running any other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T12:27:47.951092Z",
     "iopub.status.busy": "2025-11-20T12:27:47.950766Z",
     "iopub.status.idle": "2025-11-20T12:27:47.955518Z",
     "shell.execute_reply": "2025-11-20T12:27:47.954824Z",
     "shell.execute_reply.started": "2025-11-20T12:27:47.951062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"   NumPy: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:10:16.009939Z",
     "iopub.status.busy": "2025-11-19T09:10:16.009612Z",
     "iopub.status.idle": "2025-11-19T09:17:48.245626Z",
     "shell.execute_reply": "2025-11-19T09:17:48.244953Z",
     "shell.execute_reply.started": "2025-11-19T09:10:16.009909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: GENERATING CAPTIONS\n",
      "============================================================\n",
      "\n",
      "üìù Generating captions for: data/aokvqa/train_1k.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5422e60a1a44276b27d5e0a7405543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c1b9ab739e4fd9ac2e060847119a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bd2d002f5847e09be04ceeb77b18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6a6eb6638b496db73d40f0ef788ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686452f2b45448bb82ffaa74c30e2fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c928a5e919e421dbe13545d46db3b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb918cae5964788a14afdc2ed5703fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00000-of-00002-c1d24de3bacb5e0c.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [06:07<00:00, 23.19it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00001-of-00002-6b4f3abe2dc385d0.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:00<00:00, 26480.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 1000 captions\n",
      "\n",
      "üìù Generating captions for: data/aokvqa/val_200.json\n",
      "  üìÇ Reading: train-00000-of-00002-c1d24de3bacb5e0c.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [01:13<00:00, 116.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìÇ Reading: train-00001-of-00002-6b4f3abe2dc385d0.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8528/8528 [00:00<00:00, 27464.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 200 captions\n",
      "‚úÖ All captions generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: GENERATE CAPTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: GENERATING CAPTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_captions_fast(data_file, parquet_files, output_file):\n",
    "    \"\"\"Fast caption generation with GPU\"\"\"\n",
    "    print(f\"\\nüìù Generating captions for: {data_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    question_ids = set(item['question_id'] for item in data)\n",
    "    \n",
    "    # Load BLIP model\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").eval().to(device)\n",
    "    \n",
    "    captions = {}\n",
    "    \n",
    "    for pq_file in parquet_files:\n",
    "        print(f\"  üìÇ Reading: {os.path.basename(pq_file)}\")\n",
    "        df = pd.read_parquet(pq_file)\n",
    "        \n",
    "        for idx in tqdm(range(len(df)), desc=\"Generating\"):\n",
    "            row = df.iloc[idx]\n",
    "            qid = str(row['question_id'])\n",
    "            \n",
    "            if qid not in question_ids:\n",
    "                continue\n",
    "            \n",
    "            if qid in captions:\n",
    "                continue\n",
    "            \n",
    "            # Generate caption\n",
    "            img = Image.open(io.BytesIO(row['image']['bytes'])).convert('RGB')\n",
    "            inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, max_length=50, num_beams=3)\n",
    "            \n",
    "            captions[qid] = processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(captions, f)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(captions)} captions\")\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Generate captions\n",
    "if not os.path.exists('data/aokvqa/cap_train_1k.json'):\n",
    "    generate_captions_fast('data/aokvqa/train_1k.json', train_parquets, 'data/aokvqa/cap_train_1k.json')\n",
    "\n",
    "if not os.path.exists('data/aokvqa/cap_val_200.json'):\n",
    "    generate_captions_fast('data/aokvqa/val_200.json', train_parquets, 'data/aokvqa/cap_val_200.json')\n",
    "\n",
    "print(\"‚úÖ All captions generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:37.336701Z",
     "iopub.status.busy": "2025-11-20T21:33:37.336386Z",
     "iopub.status.idle": "2025-11-20T21:33:37.341104Z",
     "shell.execute_reply": "2025-11-20T21:33:37.340449Z",
     "shell.execute_reply.started": "2025-11-20T21:33:37.336679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:39.658044Z",
     "iopub.status.busy": "2025-11-20T21:33:39.657126Z",
     "iopub.status.idle": "2025-11-20T21:33:39.673303Z",
     "shell.execute_reply": "2025-11-20T21:33:39.672475Z",
     "shell.execute_reply.started": "2025-11-20T21:33:39.658016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MM-CoT model defined (Amazon Science architecture)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: ACTUAL MM-COT MODEL FROM AMAZON SCIENCE REPO\n",
    "# ============================================================================\n",
    "class UnifiedQAModel(T5ForConditionalGeneration):\n",
    "    \"\"\"\n",
    "    The actual MM-CoT model from Amazon Science's repository.\n",
    "    Integrates vision features by concatenating them with text embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__(config)\n",
    "        self.model_dim = config.d_model\n",
    "        \n",
    "        # Vision projection layer - projects ViT features to T5 dimension\n",
    "        self.vis_proj = nn.Linear(1024, config.d_model)\n",
    "        \n",
    "        # LayerNorm for stability\n",
    "        self.vis_layer_norm = nn.LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        img_features: Optional[torch.FloatTensor] = None,\n",
    "    ):\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # Encode input text\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        \n",
    "        hidden_states = encoder_outputs[0]\n",
    "        \n",
    "        # Integrate vision features\n",
    "        if img_features is not None:\n",
    "            vis_embeds = self.vis_proj(img_features)\n",
    "            vis_embeds = self.vis_layer_norm(vis_embeds)\n",
    "            \n",
    "            # Concatenate text and vision\n",
    "            hidden_states = torch.cat([hidden_states, vis_embeds], dim=1)\n",
    "            \n",
    "            # Extend attention mask\n",
    "            if attention_mask is not None:\n",
    "                batch_size = attention_mask.shape[0]\n",
    "                num_vis_tokens = vis_embeds.shape[1]\n",
    "                vis_attention_mask = torch.ones(\n",
    "                    batch_size, num_vis_tokens,\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device\n",
    "                )\n",
    "                attention_mask = torch.cat([attention_mask, vis_attention_mask], dim=1)\n",
    "        \n",
    "        # Prepare decoder inputs\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        sequence_output = decoder_outputs[0]\n",
    "        \n",
    "        if self.config.tie_word_embeddings:\n",
    "            sequence_output = sequence_output * (self.model_dim**-0.5)\n",
    "        \n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        config = T5Config.from_pretrained(pretrained_model_name_or_path)\n",
    "        model = cls(config)\n",
    "        pretrained_dict = T5ForConditionalGeneration.from_pretrained(\n",
    "            pretrained_model_name_or_path, *model_args, **kwargs\n",
    "        ).state_dict()\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {\n",
    "            k: v for k, v in pretrained_dict.items() \n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        return model\n",
    "\n",
    "print(\"‚úÖ MM-CoT model defined (Amazon Science architecture)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:45.506797Z",
     "iopub.status.busy": "2025-11-20T21:33:45.506463Z",
     "iopub.status.idle": "2025-11-20T21:33:45.518114Z",
     "shell.execute_reply": "2025-11-20T21:33:45.517276Z",
     "shell.execute_reply.started": "2025-11-20T21:33:45.506772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: PREPARING DATASETS\n",
      "============================================================\n",
      "‚úÖ Dataset ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: PREPARING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class AOKVQADataset(Dataset):\n",
    "    def __init__(self, data_file, caption_file, feature_file, tokenizer, mode='rationale'):\n",
    "        with open(data_file) as f:\n",
    "            data = json.load(f)\n",
    "        with open(caption_file) as f:\n",
    "            captions = json.load(f)\n",
    "        with open(feature_file) as f:\n",
    "            features = json.load(f)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.data = []\n",
    "        \n",
    "        for item in data:\n",
    "            qid = item['question_id']\n",
    "            img_id = str(item['image_id'])\n",
    "            \n",
    "            if img_id not in features:\n",
    "                continue\n",
    "            \n",
    "            opts = \"\\n\".join([f\"({chr(65+i)}) {c}\" for i, c in enumerate(item['choices'])])\n",
    "            \n",
    "            self.data.append({\n",
    "                'question': item['question'],\n",
    "                'caption': captions.get(qid, \"\"),\n",
    "                'options': opts,\n",
    "                'correct_idx': item.get('correct_choice_idx', -1),\n",
    "                'rationale': item['rationales'][0] if item.get('rationales') else \"\",\n",
    "                'features': features[img_id],\n",
    "                'generated_rationale': item.get('generated_rationale', \"\")\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        if self.mode == 'rationale':\n",
    "            inp = f\"Question: {item['question']}\\nCaption: {item['caption']}\\nOptions:\\n{item['options']}\\nGenerate the rationale:\"\n",
    "            out = item['rationale']\n",
    "        else:\n",
    "            inp = f\"Question: {item['question']}\\nCaption: {item['caption']}\\nOptions:\\n{item['options']}\\nRationale: {item['generated_rationale']}\\nThe answer is\"\n",
    "            out = f\" ({chr(65 + item['correct_idx'])})\"\n",
    "        \n",
    "        inp_enc = self.tokenizer(inp, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        out_enc = self.tokenizer(out, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inp_enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': inp_enc['attention_mask'].squeeze(0),\n",
    "            'labels': out_enc['input_ids'].squeeze(0),\n",
    "            'img_features': torch.tensor(item['features'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels = torch.stack([b['labels'] for b in batch])\n",
    "    labels[labels == 0] = -100\n",
    "    return {\n",
    "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'labels': labels,\n",
    "        'img_features': torch.stack([b['img_features'] for b in batch])\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Dataset ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:49.779400Z",
     "iopub.status.busy": "2025-11-20T21:33:49.779067Z",
     "iopub.status.idle": "2025-11-20T21:33:49.784700Z",
     "shell.execute_reply": "2025-11-20T21:33:49.784023Z",
     "shell.execute_reply.started": "2025-11-20T21:33:49.779377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration, T5Config, T5Tokenizer\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:33:57.991984Z",
     "iopub.status.busy": "2025-11-20T21:33:57.991251Z",
     "iopub.status.idle": "2025-11-20T21:41:29.639424Z",
     "shell.execute_reply": "2025-11-20T21:41:29.638461Z",
     "shell.execute_reply.started": "2025-11-20T21:33:57.991956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: TRAINING STAGE 1 - RATIONALE GENERATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b93b2fc5914c1b9d16bff0220142cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b70e7336bed4f4392bebe7920097d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3616633b93404ab19aab9e794a3b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edceb07e6120452da2ffcfff40e23aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1000 samples, 250 batches\n",
      "Val: 200 samples, 25 batches\n",
      "Effective batch size: 4 √ó 4 = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57ce80732f44c7c9bddba49276949b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ec04c8677f4bef91bb7dd43a8346bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b2a1781c47411a98accf429d31c64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ GPU Memory after model load:\n",
      "   Allocated: 0.29 GB\n",
      "   Reserved: 0.32 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:58<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train=20.9390, Val=14.2596\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train=11.2713, Val=7.9116\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:02<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train=7.1446, Val=6.2304\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:02<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train=6.1326, Val=5.8029\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:02<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train=5.8111, Val=5.6633\n",
      "  ‚úÖ Saved!\n",
      "‚úÖ Stage 1 complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: TRAIN STAGE 1 - RATIONALE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: TRAINING STAGE 1 - RATIONALE GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "train_ds = AOKVQADataset('/kaggle/input/coco-1k/train_1k.json', '/kaggle/input/coco-1k/cap_train_1k.json',\n",
    "                         '/kaggle/input/coco-1k/feat_train_1k.json', tokenizer, mode='rationale')\n",
    "val_ds = AOKVQADataset('/kaggle/input/coco-1k/val_200.json', '/kaggle/input/coco-1k/cap_val_200.json',\n",
    "                       '/kaggle/input/coco-1k/feat_val_200.json', tokenizer, mode='rationale')\n",
    "\n",
    "# Smaller batch size + gradient accumulation\n",
    "batch_size = 4\n",
    "accumulation_steps = 4  # Effective batch size = 4 * 4 = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples, {len(train_loader)} batches\")\n",
    "print(f\"Val: {len(val_ds)} samples, {len(val_loader)} batches\")\n",
    "print(f\"Effective batch size: {batch_size} √ó {accumulation_steps} = {batch_size * accumulation_steps}\")\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train with smaller model\n",
    "model = UnifiedQAModel.from_pretrained('google/flan-t5-small').to(device)  # Use T5-small instead\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 100, len(train_loader) * 5)\n",
    "\n",
    "print(f\"\\nüíæ GPU Memory after model load:\")\n",
    "print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"   Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\\n\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(1, 6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/5\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / accumulation_steps  # Scale loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Clear cache every few steps\n",
    "            if (i + 1) % (accumulation_steps * 4) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    avg_train = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            val_loss += model(**batch).loss.item()\n",
    "    \n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch}: Train={avg_train:.4f}, Val={avg_val:.4f}\")\n",
    "    \n",
    "    if avg_val < best_loss:\n",
    "        best_loss = avg_val\n",
    "        os.makedirs('models/rationale', exist_ok=True)\n",
    "        \n",
    "        # Save with PyTorch format (avoids NumPy 2.x issues)\n",
    "        try:\n",
    "            model.save_pretrained('models/rationale', safe_serialization=False)\n",
    "        except:\n",
    "            # Fallback: save state dict directly\n",
    "            torch.save(model.state_dict(), 'models/rationale/pytorch_model.bin')\n",
    "            model.config.save_pretrained('models/rationale')\n",
    "        \n",
    "        tokenizer.save_pretrained('models/rationale')\n",
    "        print(f\"  ‚úÖ Saved!\")\n",
    "\n",
    "print(\"‚úÖ Stage 1 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:41:45.129102Z",
     "iopub.status.busy": "2025-11-20T21:41:45.128562Z",
     "iopub.status.idle": "2025-11-20T21:41:49.564725Z",
     "shell.execute_reply": "2025-11-20T21:41:49.563971Z",
     "shell.execute_reply.started": "2025-11-20T21:41:45.129080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting safetensors\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed safetensors-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:27:05.240961Z",
     "iopub.status.busy": "2025-11-20T06:27:05.240633Z",
     "iopub.status.idle": "2025-11-20T06:27:07.000614Z",
     "shell.execute_reply": "2025-11-20T06:27:06.999880Z",
     "shell.execute_reply.started": "2025-11-20T06:27:05.240937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T06:27:35.553444Z",
     "iopub.status.busy": "2025-11-20T06:27:35.552580Z",
     "iopub.status.idle": "2025-11-20T06:27:42.023028Z",
     "shell.execute_reply": "2025-11-20T06:27:42.022213Z",
     "shell.execute_reply.started": "2025-11-20T06:27:35.553408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T21:53:51.877871Z",
     "iopub.status.busy": "2025-11-20T21:53:51.877512Z",
     "iopub.status.idle": "2025-11-20T22:04:15.949913Z",
     "shell.execute_reply": "2025-11-20T22:04:15.949266Z",
     "shell.execute_reply.started": "2025-11-20T21:53:51.877846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: GENERATING RATIONALES FOR STAGE 2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [07:19<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved to: data/aokvqa/train_1k_rat.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:35<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved to: data/aokvqa/val_200_rat.json\n",
      "‚úÖ Rationales generated!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: GENERATE RATIONALES FOR STAGE 2\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: GENERATING RATIONALES FOR STAGE 2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_rat = UnifiedQAModel.from_pretrained('models/rationale').eval().to(device)\n",
    "tokenizer_rat = T5Tokenizer.from_pretrained('models/rationale')\n",
    "\n",
    "def add_generated_rationales(data_file, feat_file, cap_file, output_file):\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    with open(feat_file) as f:\n",
    "        feats = json.load(f)\n",
    "    with open(cap_file) as f:\n",
    "        caps = json.load(f)\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Generating\"):\n",
    "        img_id = str(item['image_id'])\n",
    "        if img_id not in feats:\n",
    "            item['generated_rationale'] = \"\"\n",
    "            continue\n",
    "        \n",
    "        opts = \"\\n\".join([f\"({chr(65+i)}) {c}\" for i, c in enumerate(item['choices'])])\n",
    "        inp = f\"Question: {item['question']}\\nCaption: {caps.get(item['question_id'], '')}\\nOptions:\\n{opts}\\nGenerate the rationale:\"\n",
    "        \n",
    "        inputs = tokenizer_rat(inp, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "        vis_feat = torch.tensor([feats[img_id]], dtype=torch.float32).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model_rat.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],\n",
    "                                     img_features=vis_feat, max_length=128)\n",
    "        \n",
    "        item['generated_rationale'] = tokenizer_rat.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save to writable location\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"  ‚úÖ Saved to: {output_file}\")\n",
    "\n",
    "# Save to data/aokvqa/ (writable location)\n",
    "add_generated_rationales(\n",
    "    '/kaggle/input/coco-1k/train_1k.json',\n",
    "    '/kaggle/input/coco-1k/feat_train_1k.json',\n",
    "    '/kaggle/input/coco-1k/cap_train_1k.json',\n",
    "    'data/aokvqa/train_1k_rat.json'  # Writable location\n",
    ")\n",
    "add_generated_rationales(\n",
    "    '/kaggle/input/coco-1k/val_200.json',\n",
    "    '/kaggle/input/coco-1k/feat_val_200.json',\n",
    "    '/kaggle/input/coco-1k/cap_val_200.json',\n",
    "    'data/aokvqa/val_200_rat.json'  # Writable location\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Rationales generated!\")\n",
    "\n",
    "del model_rat, tokenizer_rat\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T22:17:10.225092Z",
     "iopub.status.busy": "2025-11-20T22:17:10.224747Z",
     "iopub.status.idle": "2025-11-20T22:24:08.439835Z",
     "shell.execute_reply": "2025-11-20T22:24:08.439024Z",
     "shell.execute_reply.started": "2025-11-20T22:17:10.225069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: TRAINING STAGE 2 - ANSWER PREDICTION\n",
      "============================================================\n",
      "Train: 1000 samples\n",
      "Val: 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:00<00:00,  4.12it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train=21.3064, Val=18.2464\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.06it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train=15.5819, Val=11.1536\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.04it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train=9.6564, Val=6.3703\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.04it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train=5.0319, Val=2.8216\n",
      "  ‚úÖ Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:01<00:00,  4.04it/s]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train=2.6071, Val=2.0943\n",
      "  ‚úÖ Saved!\n",
      "‚úÖ Stage 2 complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN STAGE 2 - ANSWER\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: TRAINING STAGE 2 - ANSWER PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_ds_ans = AOKVQADataset('data/aokvqa/train_1k_rat.json', '/kaggle/input/coco-1k/cap_train_1k.json',\n",
    "                             '/kaggle/input/coco-1k/feat_train_1k.json', tokenizer, mode='answer')\n",
    "val_ds_ans = AOKVQADataset('data/aokvqa/val_200_rat.json', '/kaggle/input/coco-1k/cap_val_200.json',\n",
    "                           '/kaggle/input/coco-1k/feat_val_200.json', tokenizer, mode='answer')\n",
    "\n",
    "train_loader_ans = DataLoader(train_ds_ans, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader_ans = DataLoader(val_ds_ans, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train: {len(train_ds_ans)} samples\")\n",
    "print(f\"Val: {len(val_ds_ans)} samples\")\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train with T5-small\n",
    "model_ans = UnifiedQAModel.from_pretrained('google/flan-t5-small').to(device)\n",
    "optimizer = AdamW(model_ans.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 100, len(train_loader_ans) * 5)\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(1, 6):\n",
    "    model_ans.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader_ans, desc=f\"Epoch {epoch}/5\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model_ans(**batch)\n",
    "        loss = outputs.loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model_ans.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (i + 1) % (accumulation_steps * 4) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    avg_train = total_loss / len(train_loader_ans)\n",
    "    \n",
    "    # Validation\n",
    "    model_ans.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader_ans, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            val_loss += model_ans(**batch).loss.item()\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_val = val_loss / len(val_loader_ans)\n",
    "    print(f\"Epoch {epoch}: Train={avg_train:.4f}, Val={avg_val:.4f}\")\n",
    "    \n",
    "    if avg_val < best_loss:\n",
    "        best_loss = avg_val\n",
    "        os.makedirs('models/answer', exist_ok=True)\n",
    "        \n",
    "        # Save with PyTorch format\n",
    "        try:\n",
    "            model_ans.save_pretrained('models/answer', safe_serialization=False)\n",
    "        except:\n",
    "            torch.save(model_ans.state_dict(), 'models/answer/pytorch_model.bin')\n",
    "            model_ans.config.save_pretrained('models/answer')\n",
    "        \n",
    "        tokenizer.save_pretrained('models/answer')\n",
    "        print(f\"  ‚úÖ Saved!\")\n",
    "\n",
    "print(\"‚úÖ Stage 2 complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T22:25:34.804946Z",
     "iopub.status.busy": "2025-11-20T22:25:34.804601Z",
     "iopub.status.idle": "2025-11-20T22:27:44.610285Z",
     "shell.execute_reply": "2025-11-20T22:27:44.609527Z",
     "shell.execute_reply.started": "2025-11-20T22:25:34.804925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8: FINAL EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:48<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ FINAL ACCURACY: 32.00% (64/200)\n",
      "\n",
      "============================================================\n",
      "‚úÖ COMPLETE PIPELINE FINISHED!\n",
      "============================================================\n",
      "\n",
      "Using ACTUAL Amazon Science MM-CoT Architecture\n",
      "Model: UnifiedQAModel with vision concatenation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_rat = UnifiedQAModel.from_pretrained('models/rationale').eval().to(device)\n",
    "model_ans = UnifiedQAModel.from_pretrained('models/answer').eval().to(device)\n",
    "tok_rat = T5Tokenizer.from_pretrained('models/rationale')\n",
    "tok_ans = T5Tokenizer.from_pretrained('models/answer')\n",
    "\n",
    "with open('/kaggle/input/coco-1k/val_200.json') as f:\n",
    "    eval_data = json.load(f)\n",
    "with open('/kaggle/input/coco-1k/feat_val_200.json') as f:\n",
    "    eval_feats = json.load(f)\n",
    "with open('/kaggle/input/coco-1k/cap_val_200.json') as f:\n",
    "    eval_caps = json.load(f)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for item in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "    img_id = str(item['image_id'])\n",
    "    if img_id not in eval_feats:\n",
    "        continue\n",
    "    \n",
    "    opts = \"\\n\".join([f\"({chr(65+i)}) {c}\" for i, c in enumerate(item['choices'])])\n",
    "    \n",
    "    # Generate rationale\n",
    "    inp1 = f\"Question: {item['question']}\\nCaption: {eval_caps.get(item['question_id'], '')}\\nOptions:\\n{opts}\\nGenerate the rationale:\"\n",
    "    inputs1 = tok_rat(inp1, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "    vis = torch.tensor([eval_feats[img_id]], dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1 = model_rat.generate(input_ids=inputs1['input_ids'], attention_mask=inputs1['attention_mask'],\n",
    "                                  img_features=vis, max_length=128)\n",
    "    rationale = tok_rat.decode(out1[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Generate answer\n",
    "    inp2 = f\"Question: {item['question']}\\nCaption: {eval_caps.get(item['question_id'], '')}\\nOptions:\\n{opts}\\nRationale: {rationale}\\nThe answer is\"\n",
    "    inputs2 = tok_ans(inp2, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out2 = model_ans.generate(input_ids=inputs2['input_ids'], attention_mask=inputs2['attention_mask'],\n",
    "                                  img_features=vis, max_length=64)\n",
    "    answer = tok_ans.decode(out2[0], skip_special_tokens=True)\n",
    "    \n",
    "    pred = next((c for c in answer if c.isalpha() and c.isupper()), '')\n",
    "    true = chr(65 + item['correct_choice_idx']) if item['correct_choice_idx'] >= 0 else ''\n",
    "    \n",
    "    if pred == true:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"\\nüéØ FINAL ACCURACY: {accuracy:.2f}% ({correct}/{total})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ COMPLETE PIPELINE FINISHED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsing ACTUAL Amazon Science MM-CoT Architecture\")\n",
    "print(\"Model: UnifiedQAModel with vision concatenation\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8743824,
     "sourceId": 13742037,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8753843,
     "sourceId": 13756467,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8753994,
     "sourceId": 13756680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8764752,
     "sourceId": 13771389,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8778833,
     "sourceId": 13789813,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8793515,
     "sourceId": 13809684,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
