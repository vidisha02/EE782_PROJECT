Overview

This repository contains our implementation of Multimodal Chain-of-Thought (MM-CoT) reasoning applied to vision-language tasks.
Our goal is to train and evaluate T5-based models capable of generating step-by-step reasoning for image-grounded questions using three widely used Visual Question Answering (VQA) datasets.
We use three different datatsets chartQA, OKVQA, A-OKVQA (all three in different folders)
